# Adaptive gradient optimization methods 

In the present project I have implemented some adaptive gradient optimization methods to minimize a 1D function that have a lot of local minimum points. The animation of the search process and the evolution of the cost (loss) function are shown.

Adaptive Gradient Optimization (AGO) methods are implemented as black box optimizers in some Machine Learning packages like TensorFlow, PyTorch, Keras, etc.

I have studied in detail some of the most important AGO methods (like Adam, Nadam, AMSGrad, RAdam) to perform the Full Waveform Inversion process, as you can see in my PhD publication: 

https://doi.org/10.1093/gji/ggaa583
